---
title: "Results"
site: workflowr::wflow_site
date: "2021-March-22"
output:
  workflowr::wflow_html:
    code_folding: "hide"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      tidy='styler', tidy.opts=list(strict=FALSE,width.cutoff=100), highlight=TRUE)
```


```{r}
library(tidyverse); library(magrittr); library(ragg); 
```

# Initial summaries

## Summary of the pedigree and germplasm

```{r}
ped<-readRDS(here::here("data","ped_awc.rds"))
ped %>% 
  count(sireID,damID) %$% summary(n)
```

```{r}
ped %>% 
  pivot_longer(cols=c(sireID,damID),names_to = "MaleOrFemale", values_to = "Parent") %>% 
  group_by(Parent) %>% 
  summarize(Ncontributions=n()) %$% summary(Ncontributions)
```

There were 3199 comprising 462 families, derived from 209 parents in our pedigree. Parents were used an average of 31 (median 16, range 1-256) times as sire and/or dam in the pedigree. The mean family size was 7 (median 4, range 1-72).

```{r}
propHom<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS14")
summary(propHom$PropSNP_homozygous)
```

The average proportion homozygous was 0.84 (range 0.76-0.93) across the 3199 pedigree members (computed over 33370 variable SNP; Table S14).

As expected for a population under recurrent selection, the homozygosity rate increases (though only fractionally) from the C0 (mean 0.826), C1 (0.835), C2 (0.838), C3 (0.839) (Figure S01).

```{r}
propHom %>% 
  mutate(Group=ifelse(!grepl("TMS13|TMS14|TMS15", GID),"GG (C0)",NA),
         Group=ifelse(grepl("TMS13", GID),"TMS13 (C1)",Group),
         Group=ifelse(grepl("TMS14", GID),"TMS14 (C2)",Group),
         Group=ifelse(grepl("TMS15", GID),"TMS15 (C3)",Group)) %>% 
  group_by(Group) %>% 
  summarize(meanPropHom=round(mean(PropSNP_homozygous),3))
```

```{r figureS01}
propHom %>% 
  mutate(Group=ifelse(!grepl("TMS13|TMS14|TMS15", GID),"GG (C0)",NA),
         Group=ifelse(grepl("TMS13", GID),"TMS13 (C1)",Group),
         Group=ifelse(grepl("TMS14", GID),"TMS14 (C2)",Group),
         Group=ifelse(grepl("TMS15", GID),"TMS15 (C3)",Group)) %>% 
  ggplot(.,aes(x=Group,y=PropSNP_homozygous,fill=Group)) + geom_boxplot() + 
  theme_bw() + 
  scale_fill_viridis_d()
```

## Summary of the cross-validation scheme

```{r}
## Table S2: Summary of cross-validation scheme
parentfold_summary<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS02")
parentfold_summary %>% 
  summarize_if(is.numeric,~ceiling(mean(.)))
```

```{r}
parentfold_summary %>% 
  summarize_if(is.numeric,~ceiling(min(.))) %>% mutate(Value="Min") %>% 
  bind_rows(parentfold_summary %>% 
              summarize_if(is.numeric,~ceiling(max(.))) %>% mutate(Value="Max"))
```

Across the 5 replications of 5-fold cross-validation the average number of samples was 1833 (range 1245-2323) for training sets and 1494 (range 1003-2081) for testing sets. The 25 training-testing pairs set-up an average of 167 (range 143-204) crosses-to-predict (Table S02).

## Summary of the BLUPs and sel. index

The correlation between phenotypic BLUPs for the two SI (stdSI and biofortSI; Table S01) was 0.43 (Figure S02). The correlation between DM and TCHART BLUPs, for which we had *a priori* expectations, was -0.29.

```{r}
library(tidyverse); library(magrittr);
# Selection weights -----------
indices<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS01")

# BLUPs -----------
blups<-readRDS(here::here("data","blups_forawcdata.rds")) %>% 
  select(Trait,blups) %>% 
  unnest(blups) %>% 
  select(Trait,germplasmName,BLUP) %>% 
  spread(Trait,BLUP) %>% 
  select(germplasmName,all_of(c("DM","logFYLD","MCMDS","TCHART")))
blups %<>% 
  select(germplasmName,all_of(indices$Trait)) %>% 
  mutate(stdSI=blups %>% 
           select(all_of(indices$Trait)) %>% 
           as.data.frame(.) %>% 
           as.matrix(.)%*%indices$stdSI,
         biofortSI=blups %>% 
           select(all_of(indices$Trait)) %>% 
           as.data.frame(.) %>% 
           as.matrix(.)%*%indices$biofortSI)
```

Correlations among phenotypic BLUPs (including Selection Indices)

```{r, fig.width=10, fig.height=5}
#```{r, fig.show="hold", out.width="50%"}
library(patchwork)
p1<-ggplot(blups,aes(x=stdSI,y=biofortSI)) + geom_point(size=1.25) + theme_bw()
corMat<-cor(blups[,-1],use = 'pairwise.complete.obs')
(p1 | ~corrplot::corrplot(corMat, type = 'lower', col = viridis::viridis(n = 10), diag = T,addCoef.col = "black")) + 
  plot_layout(nrow=1, widths = c(0.35,0.65)) +
  plot_annotation(tag_levels = 'A',
                  title = 'Correlations among phenotypic BLUPs (including Selection Indices)')
```

# Predictions accuracies

```{r}
library(tidyverse); library(magrittr);
# Table S10: Accuracies predicting the mean
accMeans<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS10")
```

## Primary analysis

### Means
```{r}
accMeansMain<-accMeans %>% 
  filter(ValidationData=="GBLUPs", grepl("DirDom",Model))
```

```{r}
accMeansMain %>% 
  select(-Model,-ValidationData) %>% 
  spread(predOf,Accuracy) %>% 
  mutate(diffAcc=MeanTGV-MeanBV) -> diffAcc
diffAcc %$% summary(diffAcc)
```
```{r}
diffAcc %>% 
  summarise(quantile = scales::percent(c(0.05,0.25, 0.5, 0.75,0.95)),
            diffAcc = quantile(diffAcc, c(0.05,0.25, 0.5, 0.75,0.95))) %>% 
  spread(quantile,diffAcc)
```

```{r}
diffAcc %>% 
  group_by(Trait) %>% 
  summarise(quantile = scales::percent(c(0.05,0.25, 0.5, 0.75,0.95)),
            diffAcc = quantile(diffAcc, c(0.05,0.25, 0.5, 0.75,0.95))) %>% 
  spread(quantile,diffAcc)
```

### (Co)variances


```{r}
library(tidyverse); library(magrittr);
## Table S11: Accuracies predicting the variances
accVars<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS11")
accVarsMain<-accVars %>% 
  filter(ValidationData=="GBLUPs", grepl("DirDom",Model),VarMethod=="PMV")
```

Across traits. Proportion of accuracy estimates above zero? Percentiles: 10%, 50% (median), 90%

```{r}
accVarsMain %>% 
  select(-Model,-ValidationData,-AccuracyCor) %>% 
  mutate(Component=ifelse(Trait1==Trait2,"Variance","Covariance"),
         TraitType=ifelse(grepl("SI",Trait1),"SI","ComponentTrait")) %>% 
  group_by(Component) %>% 
  summarize(propAboveZero=length(which(AccuracyWtCor>=0))/n(),
            quantile = scales::percent(c(0.1,0.5,0.9)),
            AccuracyWtCor = quantile(AccuracyWtCor, c(0.1,0.5,0.9))) %>% 
  pivot_wider(names_from = "quantile", values_from = "AccuracyWtCor")
```
By trait variance / trait-trait covariances.
```{r}
accVarsMain %>% 
  select(-Model,-ValidationData,-AccuracyCor) %>% 
  mutate(Component=ifelse(Trait1==Trait2,"Variance","Covariance"),
         TraitType=ifelse(grepl("SI",Trait1),"SI","ComponentTrait")) %>% 
  group_by(Component,TraitType,Trait1,Trait2) %>% 
  summarize(propAboveZero=length(which(AccuracyWtCor>=0))/n(),
            quantile = scales::percent(c(0.1,0.5,0.9)),
            AccuracyWtCor = quantile(AccuracyWtCor, c(0.1,0.5,0.9))) %>% 
  pivot_wider(names_from = "quantile", values_from = "AccuracyWtCor") %>% 
  arrange(desc(Component),TraitType,desc(`50%`))
```
Difference in accuracy between VarTGV and VarBV? 

```{r}
accVarsMain %>% 
  select(-Model,-ValidationData,-AccuracyCor) %>% 
  mutate(Component=ifelse(Trait1==Trait2,"Variance","Covariance"),
         TraitType=ifelse(grepl("SI",Trait1),"SI","ComponentTrait")) %>% 
  spread(predOf,AccuracyWtCor) %>% 
  mutate(diffAcc=VarTGV-VarBV) -> diffAcc
diffAcc %$% round(summary(diffAcc),3)
```

```{r}
diffAcc %>% 
  summarise(quantile = scales::percent(c(0.1,0.5,0.9)),
            diffAcc = quantile(diffAcc, c(0.1,0.5,0.9))) %>% 
  spread(quantile,diffAcc)
```

```{r}
diffAcc %>% 
  group_by(Component,TraitType,Trait1,Trait2) %>% 
  summarise(quantile = scales::percent(c(0.1,0.5,0.9)),
            diffAcc = quantile(diffAcc, c(0.1,0.5,0.9))) %>% 
  spread(quantile,diffAcc) %>% 
  arrange(desc(Component),TraitType,desc(`50%`))
```

### Usefulness


The usefulness criteria i.e. $UC_{parent}$ and $UC_{variety}$ are predicted by:

$$UC_{parent}=\mu_{BV} + (i_{RS} \times \sigma_{BV})$$

$$UC_{variety}=\mu_{TGV} + (i_{VDP} \times \sigma_{TGV})$$

#### Selection intensity

In order to combined predicted means and variances into a **UC**, we first calculated the realized intensity of within-family selection ($i_{RS}$ and $i_{VDP}$). For $UC_{parent}$ we computed the $i_{RS}$ based on the proportion of progeny from each family, that themselves later appeared in the pedigree as parents. For $UC_{clone}$ we compute computed $i_{VDP}$ based on the proportion of family-members that had at least one plot at the penultimate stage of the **VDP**, the advanced yield trial (AYT). For completeness and as part of exploratory analysis, we computed the proportion selected at each of the **VDP** stages: clonal evaluation trial (CET), preliminary yield trial (PYT), advanced yield trial (AYT) and uniform yield trial (UYT).  

The table below provides a quick summary of the number of families available with realized selection observed at each stage, plus the corresponding mean selection intensity and proportion selected across families.

```{r}
## Table S13: Realized within-cross selection metrics
crossmetrics<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS13")
left_join(crossmetrics %>%
            select(sireID,damID,contains("realIntensity")) %>%   distinct %>% 
            pivot_longer(cols = contains("realIntensity"),
                         names_to = "Stage", values_to = "Intensity",names_prefix = "realIntensity") %>%
            group_by(Stage) %>%
            summarize(meanIntensity=mean(Intensity, na.rm = T),
                      Nfam=length(which(!is.na(Intensity)))),
          crossmetrics %>% 
            select(sireID,damID,contains("prop")) %>%   distinct %>% 
            rename(propParent=propUsedAsParent,
                   propCET=propPhenotyped,
                   propPYT=propPastCET,
                   propAYT=propPastPYT,
                   propUYT=propPastAYT) %>% 
            pivot_longer(cols = contains("prop"),
                         values_to = "PropPast",names_to = "Stage",names_prefix = "propPast|prop") %>% 
            group_by(Stage) %>% 
            summarize(meanPropPast=mean(PropPast, na.rm = T))) %>%
  mutate(Stage=factor(Stage,levels=c("Parent","CET","PYT","AYT","UYT"))) %>% 
  arrange(Stage) %>% 
  select(Stage,Nfam,meanIntensity,meanPropPast) %>% mutate_if(is.numeric,~round(.,2))
```

There were 48 families with a mean intensity of 1.59 (mean 2% selected) that themselves had members who were parents in the pedigree. As expected, the number of available families and the proportion selected decreased (increasing selection intensity) from CET to UYT. We choose to focus on the AYT stage, which has 104 families, mean intensity 1.46 (mean 5% selected).

```{r}
library(tidyverse); library(magrittr);
## Table S9: Predicted and observed UC
predVSobsUC<-read.csv(here::here("manuscript","SupplementaryTable09.csv"),stringsAsFactors = F)

uc_cv_summary<-predVSobsUC %>% 
  filter(VarMethod=="PMV") %>% 
  group_by(Model,predOf,Stage,Trait,Repeat,Fold) %>% 
  summarize(Nfam=n(),
            meanFamSize=round(mean(FamSize),1)) %>% 
  ungroup() %>% 
  select(-Trait,-Model) %>% 
  distinct
uc_cv_summary %>% 
  group_by(predOf,Stage) %>% 
  summarize(minNfam=min(Nfam),
            meanNfam=mean(Nfam),
            maxNfam=max(Nfam),
            minMeanFamSize=min(meanFamSize),
            meanMeanFamSize=mean(meanFamSize),
            maxMeanFamSize=max(meanFamSize))
```

#### Accuracy comparisons

```{r}
library(tidyverse); library(magrittr);
## Table S12: Accuracies predicting the usefulness criteria
accUC<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS12")
accUCmain<-accUC %>% 
  filter(VarMethod=="PMV",grepl("DirDom",Model),Stage %in% c("Parent","AYT")) %>%
  select(-Model,-VarMethod,-AccuracyCor,-Stage)
# accMeansMain %>% count(Trait,predOf)
# accVarsMain %>% count(Trait1,Trait2,predOf)
# accUCmain %>% count(Repeat,Fold)
```

Proportion of UC accuracy estimates greater than zero.
```{r}
length(which(accUC$AccuracyWtCor>0))/nrow(accUC)
```

Compare Mean, Variance and UC accuracy:
```{r}
meanVsUCacc<-accUCmain %>% 
  rename(AccUC=AccuracyWtCor) %>% 
  left_join(accMeansMain %>% 
              select(-ValidationData,-Model) %>% 
              mutate(predOf=gsub("Mean","",predOf)) %>% 
              rename(AccMean=Accuracy))
meanVsUCacc %<>% 
  left_join(accVarsMain %>% 
              filter(Trait1==Trait2) %>% 
              select(-ValidationData,-Model,-AccuracyCor,-VarMethod,-Trait2) %>% 
              mutate(predOf=gsub("Var","",predOf)) %>% 
              rename(AccVar=AccuracyWtCor,
                     Trait=Trait1))
```

Correlation between mean, Var and UC accuracy, overall
```{r}
meanVsUCacc %>% 
  summarize(corMeanUCacc=cor(AccUC,AccMean),
            meanDiffMeanUC=mean(AccUC-AccMean),
            corMeanVarAcc=cor(AccVar,AccMean),
            corVarUCacc=cor(AccUC,AccVar))
```

**Correlation between mean, Var and UC accuracy, by predOf (BV vs. TGV)**
```{r}
meanVsUCacc %>% 
  group_by(predOf) %>% 
  summarize(corMeanUCacc=cor(AccUC,AccMean),
            meanDiffMeanUC=mean(AccUC-AccMean),
            corMeanVarAcc=cor(AccVar,AccMean))
```
**Correlation between mean, Var and UC accuracy, by Trait**
```{r}
meanVsUCacc %>% 
  group_by(Trait) %>% 
  summarize(corMeanUCacc=cor(AccUC,AccMean),
            meanDiffMeanUC=mean(AccUC-AccMean),
            corMeanVarAcc=cor(AccVar,AccMean)) %>% arrange(desc(corMeanUCacc))
```
**Make an overall comparison of trait accuracies for UC**
```{r, rows.print=12}
accUCmain %>% 
  mutate(TraitType=ifelse(grepl("SI",Trait),"SI","ComponentTrait")) %>% 
  group_by(Trait) %>% 
  summarize(propAboveZero=length(which(AccuracyWtCor>0))/n(),
            quantile = scales::percent(c(0.1,0.5,0.9)),
            AccuracyWtCor = quantile(AccuracyWtCor, c(0.1,0.5,0.9))) %>% 
  pivot_wider(names_from = "quantile", values_from = "AccuracyWtCor") %>% 
  mutate(across(is.numeric,~round(.,2))) %>% 
  arrange(desc(`50%`))
```

Compare UC accuracy of TGV vs. BV, overall
```{r, rows.print=12}
accUCmain %>% 
  spread(predOf,AccuracyWtCor) %>% 
  mutate(diffAcc=TGV-BV) %$% round(summary(diffAcc),2)
```

Proportion of accuracy estimates where BV>TGV.
```{r}
accUCmain %>% 
  spread(predOf,AccuracyWtCor) %>% 
  mutate(diffAcc=TGV-BV) %$% (length(which(diffAcc<=0))/length(diffAcc))
```

Compare UC accuracy difference TGV minus BV, by Trait
```{r, rows.print=12}
accUCmain %>% 
  mutate(TraitType=ifelse(grepl("SI",Trait),"SI","ComponentTrait")) %>% 
  spread(predOf,AccuracyWtCor) %>% 
  mutate(diffAcc=TGV-BV) %>% 
  group_by(Trait,TraitType) %>% 
  summarize(quantile = scales::percent(c(0.1,0.5,0.9)),
            diffAcc = quantile(diffAcc, c(0.1,0.5,0.9))) %>% 
  pivot_wider(names_from = "quantile", values_from = "diffAcc") %>% 
  mutate(across(is.numeric,~round(.,2))) %>% 
  arrange(desc(`50%`))
```

- Mean: TGV<BV (median -0.017)
- Var: No consistent / small differences
- UC: TGV<BV



# Population estimates of the importance of dominance variance 

In this study, our focus is mainly on distinguishing among crosses, and the accuracy of cross-metric predictions. Detailed analysis of the additive-dominance genetic variance-covariance structure in cassava (sub)-populations is an important topic, which we mostly leave for future study. However, we make a brief examination of the genetic variance-covariance estimates associated with the overall population and component genetic groups. We report all variance-covariance estimates in **TableS15** and complete BGLR output in the [FTP repository associated with this study](ftp://ftp.cassavabase.org/marnin_datasets/Wolfe_et_al_GenomicMateSelection).

```{r}
library(tidyverse); library(magrittr);
## Table S15: Variance estimates for genetic groups
varcomps<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS15") %>% 
  filter(VarMethod=="PMV", Method=="M2",Model %in% c("DirDomAD")) %>% 
  select(-VarMethod,-Method)
```

```{r}
varcomps %>% 
  filter(Trait1==Trait2) %>% 
  group_by(Model) %>% 
  summarize(minPropDom=min(propDom),
            meanPropDom=mean(propDom),
            maxPropDom=max(propDom))
```
Over all genetic groups analyzed, across trait and SI variances, dominance accounted for an average of 24% (range 6-53%).


```{r}
varcomps %>% 
  filter(Trait1==Trait2) %>% 
  group_by(Trait1,Trait2) %>% 
  summarize(minPropDom=min(propDom),
            meanPropDom=mean(propDom),
            maxPropDom=max(propDom)) %>% arrange(desc(meanPropDom))
```

Dominance was most important (mean 46% of genetic variance) for yield (logFYLD) and least important for TCHART (mean 11%) (Figure 4).

# Population estimates of inbreeding effects

```{r}
## Table S16: Directional dominance effects estimates
ddEffects<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS16")
ddEffects %>% 
  select(-Repeat,-Fold,-InbreedingEffectSD) %>% 
  mutate(Dataset=ifelse(Dataset!="GeneticGroups",Group,Dataset)) %>% 
  select(-Group) %>% 
  group_by(Dataset,Trait) %>% 
  summarize_all(~round(mean(.),3)) %>% 
  spread(Dataset,InbreedingEffect)
```

We found mostly consistent and significant (diff. from zero) effects of inbreeding *depression* associated especially (**Figure 5**, **Table S16**), with logFYLD (mean effect -2.75 across genetic groups, -3.88 across cross-validation folds), but also DM (-4.82 genetic groups, -7.85 cross-validation) and MCMDS (0.32 genetic groups, 1.27 cross-validation). This corresponds to higher homozygosity being associated with lower DM, lower yield and worse disease.


# Exploring Untested Crosses

We made 8 predictions (2 SIs x 2 prediction targets [BV, TGV] x 2 criteria [Mean, UC = Mean + 2\*SD]) prediction for each of 47,083 possible crosses of 306 parents.

## Correlations among predictions

First, quickly evaluate the multivariate decision space encompassed by predictions of mean, SD, UC for BV and TGV.

```{r}
library(tidyverse); library(magrittr); 
predUntestedCrosses<-read.csv(here::here("manuscript","SupplementaryTable18.csv"),stringsAsFactors = F) %>% 
  filter(Model=="DirDom") %>% select(-Model)
```

Average correlations between BiofortSI and StdSI by prediction.
```{r}
predUntestedCrosses %>% 
  spread(Trait,Pred) %>% 
  group_by(PredOf,Component) %>% 
  summarize(corSelIndices=cor(stdSI,biofortSI)) %>% 
  group_by(PredOf) %>% 
  summarize(meanCorSIs=mean(corSelIndices))
```

**TABLE:** Correlations between predictions about each selection index ($\overset{StdSI,BiofortSI}{\textbf{cor}}$).
```{r}
predUntestedCrosses %>% 
  spread(Trait,Pred) %>% 
  group_by(PredOf,Component) %>% 
  summarize(corSelIndices=cor(stdSI,biofortSI)) %>% 
  spread(Component,corSelIndices) %>% 
  arrange(PredOf) %>% 
  rmarkdown::paged_table()
```

**TABLE:** Correlations between predictions about each component, for each trait ($\overset{BV,TGV}{\textbf{cor}}$).

```{r}
predUntestedCrosses %>% 
  spread(Component,Pred) %>% 
  group_by(Trait,PredOf) %>% 
  summarize(corBV_TGV=round(cor(BV,TGV),2)) %>% 
  spread(Trait,corBV_TGV)
```

```{r}
predUntestedCrosses %>% 
  spread(PredOf,Pred) %>% 
  group_by(Trait,Component) %>% 
  summarize(corMeanSD=round(cor(Mean,Sd),2),
            corMeanUC=round(cor(Mean,UC),2),
            corSdUC=round(cor(Sd,UC),2)) %>% ungroup() %>% 
summarize(across(is.numeric,mean))#corComponents=round(cor(BV,TGV),2)) %$% summary(corComponents) 
```

```{r}
predUntestedCrosses %>% 
  spread(PredOf,Pred) %>% 
  group_by(Trait,Component) %>% 
  summarize(corMeanSD=round(cor(Mean,Sd),2),
            corMeanUC=round(cor(Mean,UC),2),
            corSdUC=round(cor(Sd,UC),2)) %>% 
  rmarkdown::paged_table()
```

The mean and variance have a low, but negative correlation. At the standardized intensity of 2.67 (1% selected), leads to a small negative correlation between SD and UC. The crosses with highest mean will mostly be those with highest UC. The crosses with highest mean will also have a small tendency to have smaller variance.

Nevertheless, the **biggest differences** in decision space have to do with the difference between using the Mean vs. including the SD via the UC.


## Decision space - top 50 crosses?

What we next want to know, is how different the selections of crosses-to-make would be if we use different criteria, particularly the mean vs. the UC.

For each of the 8 predictions of 47,083 crosses, select the top 50 ranked crosses.

```{r}
top50crosses<-predUntestedCrosses %>% 
  filter(PredOf!="Sd") %>%
  group_by(Trait,PredOf,Component) %>% 
  slice_max(order_by = Pred,n=50) %>% ungroup()
```

```{r}
top50crosses %>% distinct(sireID,damID) %>% nrow()
```

Number of distinct crosses selected per Trait
```{r}
top50crosses %>% 
  distinct(Trait,sireID,damID) %>% 
  count(Trait)
```

Number of Self vs. Outcross selected by Trait
```{r}
top50crosses %>% 
  distinct(Trait,sireID,damID,IsSelf) %>% 
  count(Trait,IsSelf)
```

Only 202 unique crosses selected based on at least one of the 8 criteria. Of those 112 were selected for the StdSI (90 Biofort) including 7 (6) selfs on the StdSI (BiofortSI). 

```{r}
top50crosses %>% 
  distinct(Trait,sireID,damID) %>% 
  mutate(Selected="Yes") %>% 
  spread(Trait,Selected) %>% 
  na.omit(.)
```
There were 0 crosses selected for both SI. 

```{r}
top50crosses %>% 
  distinct(CrossPrevMade,sireID,damID) %>% 
  count(CrossPrevMade)
```
None of the selected crosses have previously been tested.

**Table:** Summarize, by trait, the number of and relative contributions (number of matings) proposed for each parent selected in the group of top crosses.
```{r}
top50crosses %>% 
  mutate(Family=paste0(sireID,"x",damID)) %>% 
  select(Trait,Family,sireID,damID) %>% 
  pivot_longer(cols = c(sireID,damID), names_to = "Parent", values_to = "germplasmName") %>% 
  count(Trait,germplasmName) %>% 
  group_by(Trait) %>% 
  summarize(Nparents=length(unique(germplasmName)),
            minProg=min(n),maxProg=max(n),medianProg=median(n))
```

**Next:** For each SI, break down the criteria for which the "best" crosses are interesting.

Quantify the number of unique crosses selected by:

**Component (BV vs. TGV)**
   
  * Not many crosses are selected for both their BV _and_ TGV?
  * Selfs get selected mostly by TGV???

**Table:** Number of crosses for each SI selected by BV, TGV or both, ignoring selection based Mean vs. UC.
```{r}
top50crosses %>% 
  distinct(Trait,sireID,damID,Component) %>% 
  mutate(Selected="Yes") %>% 
  spread(Component,Selected) %>% 
  mutate(across(everything(.),replace_na,replace = "No")) %>% 
  count(Trait,BV,TGV) %>% spread(Trait,n) %>% 
  rmarkdown::paged_table()
```

**Table:** Number of crosses for each SI selected by BV, TGV or both, ignoring selection based Mean vs. UC. Broken down by whether
```{r}
top50crosses %>% filter(IsSelf==TRUE) %>% 
  distinct(Trait,sireID,damID,Component) %>% 
  mutate(Selected="Yes") %>% 
  spread(Component,Selected) %>% 
  mutate(across(everything(.),replace_na,replace = "No")) %>% 
  count(Trait,BV) %>% 
  rmarkdown::paged_table()
```
Selfs were _only_ selected based on BV.

**Table:** Compute the number of parents unique selected based on BV vs. TGV
```{r}
top50crosses %>% 
  nest(families=c(-Trait,-Component)) %>% 
  spread(Component,families) %>% 
  mutate(NparentsBVunique=map2_dbl(BV,TGV,~length(union(.x$sireID,.x$damID) %>% .[!. %in% union(.y$sireID,.y$damID)])),
         NparentsTGVunique=map2_dbl(BV,TGV,~length(union(.y$sireID,.y$damID) %>% .[!. %in% union(.x$sireID,.x$damID)])),
         NparentsTot=map2_dbl(BV,TGV,~length(unique(c(.x$sireID,.x$damID,.y$sireID,.y$damID))))) %>% 
  select(-BV,-TGV) %>% arrange(Trait) %>% rmarkdown::paged_table()
```

**Table:** Compute the number of parents uniquely selected based on Mean, UC and both
```{r}
top50crosses %>% 
  distinct(Trait,sireID,damID,PredOf) %>% 
  mutate(Selected="Yes") %>% 
  spread(PredOf,Selected) %>% 
  mutate(across(everything(.),replace_na,replace = "No")) %>% 
  count(Trait,Mean,UC) %>%spread(Trait,n) %>% 
  rmarkdown::paged_table()
```

## So which are the "BEST" crosses? 
   
* Chosen most times, for most criteria?
  
```{r}
best50crosses<-top50crosses %>% 
  count(sireID,damID,Trait) %>% 
  group_by(Trait) %>% 
  slice_max(order_by = `n`,n = 50, with_ties = TRUE) %>% 
  rename(NtimesChosen=n) 
best50crosses %>% count(Trait)
```
If you use the most times chosen as the criteria and you don't break ties, there are 58 StdSI and 66 BiofortSI crosses to consider as the "best".

```{r}
### Are these crosses really "better"?
# bind_rows(predUntestedCrosses %>% 
#             filter(CrossPrevMade=="Yes") %>% 
#             mutate(Group=ifelse(grepl("TMS13",sireID) & grepl("TMS13",damID),"C1",
#                                 ifelse(grepl("TMS14",sireID) & grepl("TMS14",damID),"C2",
#                                        ifelse(grepl("TMS15",sireID) & grepl("TMS15",damID),"C3","C0")))),
#           predUntestedCrosses %>% 
#             semi_join(best50crosses) %>% 
#             mutate(Group="NewCrosses")) %>% 
#   group_by(Group,Trait,PredOf,Component) %>% 
#   summarize(meanPred=mean(Pred),
#             sePred=sd(Pred)/sqrt(n())) %>% 
#   ggplot(.,aes(x=Group,y=meanPred,fill=Component)) + 
#   facet_grid(PredOf~Trait, scales = 'free') + 
#   geom_bar(stat = 'identity', position='dodge')
```

# Supplementary Questions

## Does the validation type make a difference?

Most often, cross-validation done to test genomic prediction accuracy uses validation data (the stand-in for "truth") consisting of adjusted values, (e.g. BLUPs or BLUEs) for total individual performance, not including genomic relatedness information. In our study we set-up cross-validation folds that enable use to predict the GEBV and GETGV (GBLUPs) of validation family-members, and to subsequently compute their sample means, variances and usefulness. This approach has the added advantage of expanding the available sample size of validation progeny with complete data across traits. Nevertheless, we made some comparison to results using BLUPs that do not incorporate genomic relatedness information; in other words, independent and identically distributed (i.i.d.) BLUPs.

```{r}
library(tidyverse); library(magrittr);
# Table S10: Accuracies predicting the mean
accMeans<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS10")
## Table S11: Accuracies predicting the variances
accVars<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS11") %>% 
  mutate(Component=ifelse(Trait1==Trait2,"Variance","Covariance"),
         TraitType=ifelse(grepl("SI",Trait1),"SI","ComponentTrait"))
```
**Summarize accuracy differences between GBLUPs and iidBLUPs as validation data.**
```{r}
accMeans %>% 
  filter(grepl("DirDom",Model)) %>% 
  spread(ValidationData,Accuracy) %>% 
  mutate(diffAcc=GBLUPs-iidBLUPs) %$% summary(diffAcc)
```
Prediction accuracy for family means were nearly uniformly higher using GBLUPs compared to iidBLUPs (median 0.18 higher). 

**Compute Spearman rank correlations on a per Trait, per prediction type (BV vs TGV) basis.**
```{r}
accMeans %>% 
  filter(grepl("DirDom",Model)) %>% 
  spread(ValidationData,Accuracy) %>% 
  group_by(Trait,predOf) %>% 
  summarize(corAcc=cor(GBLUPs,iidBLUPs,method = 'spearman')) %$% summary(corAcc) # arrange(desc(corAcc))
```
The Spearman rank correlation between prediction accuracies based on iidBLUPs and GBLUPs was high (median 0.75, range 0.55-0.84). 

**Summarize accuracy differences between GBLUPs and iidBLUPs as validation data.**
```{r}
accVars %>% 
  filter(grepl("DirDom",Model),VarMethod=="PMV") %>% 
  select(-AccuracyCor) %>% 
  spread(ValidationData,AccuracyWtCor) %>% 
  mutate(diffAcc=GBLUPs-iidBLUPs) %$% summary(diffAcc)
```
Similar to the means, accuracy using GBLUP-validation-data appeared mostly higher compared to iidBLUPs (median difference GBLUPs-iidBLUPs = 0.07, interquartile range -0.002-0.14).

**Compute Spearman rank correlations on a per Trait, per prediction type (BV vs TGV) basis.**
```{r}
accVars %>% 
  filter(grepl("DirDom",Model),VarMethod=="PMV") %>% 
  select(-AccuracyCor) %>% 
  spread(ValidationData,AccuracyWtCor) %>% 
  group_by(Component,TraitType,Trait1,Trait2,predOf) %>% 
  summarize(corAcc=cor(GBLUPs,iidBLUPs,method = 'spearman')) %$% summary(corAcc) # arrange(desc(corAcc))
```

The Spearman rank correlations of iidBLUP and GBLUP-validation-based accuracies was positive for family (co)variances, but smaller compared to family means (mean correlation 0.5, range 0.04-0.89). 

Supplementary plots comparing validation-data accuracies for means and (co)variances were inspected (**Figure S??-S??** [Figure ???](SupplementaryFigures.html)). Based on this, we conclude that we would reach similar though more muted conclusions about which trait variances and trait-trait covariances are best or worst predicted, if restricted to iidBLUPs for validation data.

## Accuracy considering only families with 10+ members?

```{r, eval=T}
library(tidyverse); library(magrittr);
obsVSpredVars<-readRDS(here::here("output","obsVSpredVars.rds")) %>% 
  filter(grepl("DirDom",Model),ValidationData=="GBLUPs",VarMethod=="PMV") %>% 
  select(-Model,-ValidationData,-VarMethod)
```
```{r}
obsVSpredVars %>% 
  distinct(sireID,damID,FamSize) %>% 
  filter(FamSize>=10) %>% nrow()
```
```{r}
obsVSpredVars %>% 
  distinct(sireID,damID,FamSize) %>% 
  filter(FamSize>=20) %>% nrow()
```

In our primary analysis, we computed (co)variance prediction accuracies with weighted correlations, considering any family with more than one member. We also considered a more conservative alternative approach of including only families with $\geq$10 (n=112) or $\geq$20 (n=22). 

Family Size >= 20
```{r, eval=T}
# Variances
compareAllFamsToBigFamsAccuracy<-obsVSpredVars %>% 
  drop_na(.) %>% 
  filter(FamSize>=20) %>% 
  select(-FamSize,-Nobs) %>% 
  nest(predVSobs=c(sireID,damID,predVar,obsVar,CorrWeight)) %>% 
  mutate(FamSize10plus=map_dbl(predVSobs,~psych::cor.wt(.[,3:4],w = .$CorrWeight) %$% r[1,2])) %>% 
  select(-predVSobs) %>% 
  left_join(readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS11") %>% 
              filter(VarMethod=="PMV",ValidationData=="GBLUPs", grepl("DirDom",Model)) %>% 
              select(-Model,-ValidationData,-VarMethod) %>% 
              select(-AccuracyCor) %>% 
              rename(AllFams=AccuracyWtCor)) %>% 
  mutate(accDiff=FamSize10plus-AllFams)
compareAllFamsToBigFamsAccuracy$accDiff %>% summary
```
Family Size >= 20
```{r}
compareAllFamsToBigFamsAccuracy %>% 
  summarize(corAllVSBig=cor(FamSize10plus,AllFams,method = 'spearman',use='pairwise.complete.obs'))
```
Family Size >= 10
```{r}
# Variances
compareAllFamsToBigFamsAccuracy<-obsVSpredVars %>% 
  drop_na(.) %>% 
  filter(FamSize>=10) %>% 
  select(-FamSize,-Nobs) %>% 
  nest(predVSobs=c(sireID,damID,predVar,obsVar,CorrWeight)) %>% 
  mutate(FamSize10plus=map_dbl(predVSobs,~psych::cor.wt(.[,3:4],w = .$CorrWeight) %$% r[1,2])) %>% 
  select(-predVSobs) %>% 
  left_join(readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS11") %>% 
              filter(VarMethod=="PMV",ValidationData=="GBLUPs", grepl("DirDom",Model)) %>% 
              select(-Model,-ValidationData,-VarMethod) %>% 
              select(-AccuracyCor) %>% 
              rename(AllFams=AccuracyWtCor)) %>% 
  mutate(accDiff=FamSize10plus-AllFams)
compareAllFamsToBigFamsAccuracy$accDiff %>% summary
```
Family Size >= 10
```{r}
compareAllFamsToBigFamsAccuracy %>% 
  summarize(corAllVSBig=cor(FamSize10plus,AllFams,method = 'spearman',use='pairwise.complete.obs'))
```
```{r, rows.print=12}
compareAllFamsToBigFamsAccuracy %>% 
  group_by(Trait1,Trait2) %>% 
  summarize(medianDiffAccSize10PlusVsAllFams=round(median(FamSize10plus-AllFams),3)) %>% arrange(desc(medianDiffAccSize10PlusVsAllFams))
```
The Spearman rank correlation between accuracy estimates when all vs. only families with more than 10 members was 0.89. There should therefore be good concordance with our primary conclusions, depending on the family size threshold we impose. 

The median difference in accuracy ("threshold size families" minus "all families") was 0.01. Considering only size 10 or greater families noticeably improved prediction accuracy for several trait variances and especially for two covariances (DM-TCHART and logFYLD-MCMDS) (**Figures S???**). 
 
## Comparing PMV and VPM predictions

Comparing posterior mean variance (PMV) to variance of posterior mean (VPM) predictions: 

Variances and covariances estimates and cross-validation predictions were made with the PMV method, which is computationally more intensive than VPM. 

**Overall Spearman rank correlation between VPM and PMV variance component estimates.**
```{r}
library(tidyverse); library(magrittr);
## Table S15: Variance estimates for genetic groups
varcomps<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS15")
varcomps %>% 
  filter(Method=="M2",grepl("DirDomAD",Model)) %>% 
  select(-propDom,-Model,-Method) %>% 
  pivot_longer(cols = c(VarA,VarD), names_to = "VarComp", values_to = "VarEst") %>% 
  filter(!is.na(VarEst)) %>% 
  spread(VarMethod,VarEst) %>% 
  summarize(corPMV_VPM=cor(PMV,VPM, use = 'pairwise.complete.obs',method = 'spearman'))
```
We observed that population variance estimates based on PMV were consistently larger than VPM, but the correlation of those estimates is 0.98 (**Figure __**). 

**Compute the correlation between PMV and VPM predictions.**
```{r}
obsVSpredVars<-readRDS(here::here("output","obsVSpredVars.rds")) %>% 
  filter(grepl("DirDom",Model),ValidationData=="GBLUPs") %>% 
  select(-ValidationData,-Model)

obsVSpredVars %>% 
  select(-obsVar,-FamSize,-Nobs,-CorrWeight) %>% 
  spread(VarMethod,predVar) %>% 
  #group_by(predOf) %>% 
  summarize(corPMV_VPM=cor(PMV,VPM, use = 'pairwise.complete.obs',method = 'spearman'))
```
Using the predictions from the cross-validation results, we further observed that the PMV predictions were consistently larger and most notably that the correlation between PMV and VPM was very high (0.995). Some VPM prediction accuracies actually appear better than PMV predictions (**Figure S???**). 

The critical point is that VPM and PMV predictions should have very similar rankings. In our primary analysis, we focus on the PMV results with the only exception being the exploratory predictions where we saved time/computation and used the VPM. If implementing mate selections via the usefulness criteria, choosing the VPM method would mostly have the consequence of shrinking the influence on selection decisions towards the mean. 

## Compare directional dominance to "classic" model

Our focus in this article was not in finding the optimal or most accurate prediction model for obtaining marker effects. However, genome-wide estimates of directional dominance have not previously been made in cassava. For this reason, we make some brief comparison to the standard or “classic” additive-dominance prediction model, where dominance effects are centered on zero.

```{r}
library(tidyverse); library(magrittr);
# Table S10: Accuracies predicting the mean
accMeans<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS10") %>% 
  filter(ValidationData=="GBLUPs") %>% 
  select(-ValidationData) %>% 
  mutate(Model=ifelse(grepl("DirDom",Model),"DirDom","Classic"))
## Table S11: Accuracies predicting the variances
accVars<-readxl::read_xlsx(here::here("manuscript","SupplementaryTables.xlsx"),sheet = "TableS11") %>% 
  mutate(Component=ifelse(Trait1==Trait2,"Variance","Covariance"),
         TraitType=ifelse(grepl("SI",Trait1),"SI","ComponentTrait")) %>% 
  filter(ValidationData=="GBLUPs",VarMethod=="PMV") %>% 
  select(-ValidationData,-VarMethod,-AccuracyCor) %>% 
  mutate(Model=ifelse(grepl("DirDom",Model),"DirDom","Classic"))

```

**Compute rank correlations overall (DirDom vs Classic): Means**
```{r}
accMeans %>% 
  spread(Model,Accuracy) %>%   
  summarize(corMeanAcc=cor(DirDom,Classic,method = 'spearman'))
```
**Compute rank correlations overall (DirDom vs Classic): Variances**
```{r}
accVars %>% 
  spread(Model,AccuracyWtCor) %>% 
  summarize(corVarAcc=cor(DirDom,Classic,method = 'spearman'))
```
Overall, the ranking of models and predictions between the two models were similar, as indicated by a rank correlation between model accuracy estimates of 0.98 for family means and 0.94 for variances and covariances. 

**Summarize accuracy differences between DirDom and Classic model: Means**
```{r}
accMeans %>% 
  spread(Model,Accuracy) %>% 
  mutate(diffAcc=DirDom-Classic) %$% summary(diffAcc)
```
**Summarize accuracy differences between DirDom and Classic model: Variances**
```{r}
accVars %>% 
  spread(Model,AccuracyWtCor) %>% 
  mutate(diffAcc=DirDom-Classic) %$% summary(diffAcc)
```

Three-quarters of family-mean and almost half of (co)variance accuracy estimates were higher using the directional dominance model. 

```{r}
# **Compute rank correlations of models - cor(DirDom,Classic) - by Trait, predOf - Means**
# accMeans %>% 
#   spread(Model,Accuracy) %>%   
#   group_by(Trait,predOf) %>% 
#   summarize(corAcc=cor(DirDom,Classic,method = 'spearman')) %>% spread(predOf,corAcc) # %$% summary(corAcc) # arrange(desc(corAcc))


# **Compute rank correlations of models - cor(DirDom,Classic) - by Trait, predOf - Variances**
#  accVars %>% 
#   spread(Model,AccuracyWtCor) %>% 
#   group_by(Component,TraitType,Trait1,Trait2,predOf) %>% 
#   summarize(corAcc=round(cor(DirDom,Classic,method = 'spearman'),3),
#             medianDiffAcc=round(median(DirDom-Classic),3)) %>% arrange(corAcc) %>% rmarkdown::paged_table()
```

The most notably improved predictions were for the family-mean logFYLD TGV (**Figure S?? and S??**).


**Overall correlations between all Classic and Directional Dominance Predictions of Untested Crosses**
```{r}
library(tidyverse); library(magrittr); 
predUntestedCrosses<-read.csv(here::here("manuscript","SupplementaryTable18.csv"),stringsAsFactors = F) 
# predUntestedCrosses %>% 
#   spread(Model,Pred) %>% 
#   group_by(Trait,PredOf,Component) %>% 
#   summarize(corModels=round(cor(ClassicAD,DirDom,method = 'spearman'),2)) %>% 
#   spread(Component,corModels) %>% 
#   arrange(Trait,PredOf) %>% 
#   rmarkdown::paged_table()
predUntestedCrosses %>% 
  spread(Model,Pred) %>% 
  summarize(corModels=round(cor(ClassicAD,DirDom,method = 'spearman'),2))
```
There was also an overall rank correlation of 0.98 between models in the prediction of untested crosses. 

## Convergence

Visual check of the posterior distribution and trace of error variance estimates, to check convergence.

```{r, eval=T}
library(tidyverse); library(magrittr); library(patchwork); library(ragg)
cv_brrIterHist<-list.files(here::here("output/mtMarkerEffects")) %>% 
  grep(".dat",.,value = T) %>% 
  grep("Repeat",.,value = T) %>% 
  tibble(Filename=.) %>% 
  separate(col = Filename,
           into = c("prefix","rep","fold","dataset","Model","extra"),
           sep = "_",remove = F) %>% 
  mutate(Dataset=paste(rep,fold,dataset,Model,sep = "_")) %>% 
  select(Dataset,Model,Filename)
gengroup_brrIterHist<-list.files(here::here("output/mtMarkerEffects")) %>% 
  grep(".dat",.,value = T) %>% 
  grep("Repeat",.,value = T, invert = T) %>% 
  tibble(Filename=.) %>% 
  separate(col = Filename,
           into = c("prefix","Dataset","Model","extra"),
           sep = "_",remove = F) %>% 
  mutate(Dataset=paste0(Dataset,"_",Model)) %>% 
  select(Dataset,Model,Filename)

brrIterHist<-bind_rows(gengroup_brrIterHist,cv_brrIterHist) %>% 
  mutate(iterHist=map(Filename,~read.table(paste0(here::here("output/mtMarkerEffects/"),.),
                                            header = F, stringsAsFactors = F) %>% 
                        mutate(Iteration=1:nrow(.)))) %>% 
  unnest() %>% 
  pivot_longer(cols=c(V1:V10),values_to = "varR",names_to = "varCovarParam")
# brrIterHist$iterHist[[1]] %>% slice()
# brrIterHist %>% slice(1) %>% unnest(iterHist)
```

```{r,warning=F}
pngfile <- fs::path(knitr::fig_path(),  "figureS2.png")
#agg_png(pngfile, width = 17.8, height = 11.13, units = "cm", res = 300, scaling = 0.9)
agg_png(pngfile, width = 12.7, height = 15, units = "cm", res = 300, scaling = 0.9)

colors<-viridis::viridis(4)[1:3]

brrIterHist %>% 
  ggplot(.,aes(x=Iteration,y=varR,color=Model,group=Dataset)) + 
  geom_line(alpha=0.7,size=0.8) + 
  theme_bw() +
  facet_grid(varCovarParam~., scales = 'free_y') + 
 # facet_wrap(~varCovarParam,scales = 'free_y',ncol=1) + 
  theme_bw() +  
  theme(plot.title = element_text(face='bold'),
        axis.title.x = element_blank(), #element_text(face='bold',color = 'black'),
        axis.title.y = element_text(face='bold',color = 'black'),
        strip.text.x = element_text(face='bold',color='black'),
        axis.text.x = element_text(color='black'),
        axis.text.y = element_text(color='black'),
        legend.title = element_text(face = 'bold',color='black'),
        legend.text = element_text(face='bold'),
        legend.position = 'bottom',
        strip.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(values=colors) + 
  labs(x="Thinned Iteration", y = "Error (Co)variance")
invisible(dev.off())
knitr::include_graphics(pngfile)
```
```{r}
brrIterHist %>% 
  arrange(Dataset,Model,varCovarParam,Iteration) %>% 
  group_by(Dataset,Model,varCovarParam) %>% 
  slice(1001:6000) %>% 
  ungroup() %>% 
  arrange(Dataset,Model,varCovarParam,Iteration) -> x
```

```{r,warning=F}
pngfile <- fs::path(knitr::fig_path(),  "figureS2.png")
agg_png(pngfile, width = 17.8, height = 11.13, units = "cm", res = 300, scaling = 0.9)

colors<-viridis::viridis(10)
x %>%   
  ggplot(.,aes(x=varR,fill=Model)) + 
  geom_density(alpha=0.7,color='black') + 
  theme_bw() +
#  facet_grid(varCovarParam~Model, scales = 'free') + 
  facet_wrap(~varCovarParam,scales = 'free') + 
  theme_bw() +  
  theme(plot.title = element_text(face='bold'),
        axis.title.x = element_blank(), #element_text(face='bold',color = 'black'),
        axis.title.y = element_text(face='bold',color = 'black'),
        strip.text.x = element_text(face='bold',color='black'),
        axis.text.x = element_text(face = 'bold',color='black', angle=30, hjust=1),
        axis.text.y = element_text(face = 'bold',color='black'),
        legend.title = element_text(face = 'bold',color='black'),
        legend.text = element_text(face='bold'),
        legend.position = 'bottom',
        strip.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_color_manual(values=colors) + 
  labs(title="Distribution of Post burn-in Error (co)variance estimates")
invisible(dev.off())
knitr::include_graphics(pngfile)
```
